{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. About the Kernel"},{"metadata":{},"cell_type":"markdown","source":"In this Kernel we will be exploring data set provided for Forest Cover type classification in Beginner's playground competition and apply different classifier algorithm ,will evaluate each algorithm.\n\n*This is Work in Progress Kernel ,will update on regular basis*\n\nTo get more on EDA refer my another [kernel](https://www.kaggle.com/kushbhatnagar/first-competition-kernel-house-pricing-prediction) on House Sale Price Prediction Competition \n\nIf you like the kernel please upvote :)"},{"metadata":{},"cell_type":"markdown","source":"# 2. Get the Dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing the required libraries and data set \nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train=pd.read_csv(\"../input/learn-together/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking data set details like number of records , number of columns , column data type"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking first few rows\ndataset_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total number of records\ndataset_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column Details\ndataset_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create feature matrix , will keep all columns except 'Id'\nX=dataset_train.drop(columns=['Id'])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Analysis"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's examin relationship between differnet variables and cover type"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(X['Elevation'],X['Aspect'],hue=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Scatterplot is not give us any clear picture , let's try box plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between elevation and Cover type\nsns.boxplot(y=X['Elevation'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Aspect and Cover type\nsns.boxplot(y=X['Aspect'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Slope and Cover type\nsns.boxplot(y=X['Slope'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Horizontal_Distance_To_Hydrology and Cover type\nsns.boxplot(y=X['Horizontal_Distance_To_Hydrology'],x=X['Cover_Type'],palette='rainbow')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Vertical_Distance_To_Hydrology and Cover type\nsns.boxplot(y=X['Vertical_Distance_To_Hydrology'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Horizontal_Distance_To_Roadways and Cover type\nsns.boxplot(y=X['Horizontal_Distance_To_Roadways'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Hillshade_9am and Cover type\nsns.boxplot(y=X['Hillshade_9am'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Hillshade_Noon and Cover type\nsns.boxplot(y=X['Hillshade_Noon'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Hillshade_3pm and Cover type\nsns.boxplot(y=X['Hillshade_3pm'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot between Horizontal_Distance_To_Fire_Points and Cover type\nsns.boxplot(y=X['Horizontal_Distance_To_Fire_Points'],x=X['Cover_Type'],palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above box plots gives us fair understanding that for each forest cover type there are entries in every variables so for now we will consider all these variables in our analysis . Remaining variables are binary variables , which means they are already label encoded (*this makes our work little Simpler)*\n\nFor now we will group similar varaibles and try to understand relationship across  differnt forest cover types. We can divide variables in follwoing categories \n* **Degree Variables** : We can consider variables *'Elevation,'Aspect' and 'Slope'* under this category , as these three variable are about  measurments either in angular or numerical form\n* **Distance Variables** : We can consider following variables as they are about different distances from varoius points *'Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points*\n* **Hillsahde Variables** : Three hillshade variables comes under this cateogry\n* **Wilderness Variables** : All four wilderness variables comes under this category\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for Degree Variables \nX_deg=X[['Elevation','Aspect','Slope','Cover_Type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating pairplot for Degree Variables\nsns.pairplot(X_deg,hue='Cover_Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above pairplots we can say that for forest cover types '1' and '7' elevation value lies between '2500' and 4000' and for forest cover type '2' elevation value lies between '2000' and '3500'.\n\nFor 'Aspect' and 'Slope' each forest cover type has almost equal distribution. So, we can say 'Elevation' can play a role in classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for Distance Variables \nX_dist=X[['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points','Cover_Type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating pairplot for Degree Variables\nsns.pairplot(X_dist,hue='Cover_Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's focus on pairplots between variables 'Horizaontal/Vertical_Distance_To_Hydrology' and Cover Type. \n\nFor cover type *'3','4' and '6*'   distances are not going to upper values and for * '1','2' ,'5' and '7'*   it's going to higher range.\n\nLet's check 'Horizontal_Distance_To_Rodways/Fire_Points'. Here also, for cover type  *'3','4' and '6' *  distances are not going to upper values and for  *'1','2' ,'5' and '7'*   it's going to higher range\n\nIt's  quiet evident that these distances are playing role in classification of forest cover type"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for Hillshade Variables \nX_hs=X[['Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Cover_Type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating pairplot for Hillshade Variables\nsns.pairplot(X_hs,hue='Cover_Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graphs it's evident that 'Hillshade_9am' and 'Hillshade_Noon' have differnt ranges of start index for all forest cover types . Where as , 'Hillshade_3pm' gives almost same ranges for all forest cover type. We can consider them in our analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for Hillshade Variables \nX_wild=X[['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4','Cover_Type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating pairplot for Hillshade Variables\nsns.pairplot(X_wild,hue='Cover_Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Cover Type '2' are spread across all four wilderness area , cover types '1' , '7' are in three wilderness area while , '5' , '6' are in two and '4' , '3' are in one wilderness area\n\nIt's clear that  Wilderness areas variables are important in forest cover type classifications\n\nFor 'Soil Type' variables , we will consider them all because as per the data description it looks like they can play role in forest cover type classification."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Conclusion** For our model training we will be considering all variables present in data set . I do have feeling that we can skip either 'Aspect' or 'Slope' variables based on the pairplot which we have plotted in previous section but let's keep them both for now , we can see our results and then decide regarding this."},{"metadata":{},"cell_type":"markdown","source":"# 4. Missing Data"},{"metadata":{},"cell_type":"markdown","source":"Let's check missing values in our independent and dependent variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking missing values \ntotal_missing_values_X=X.isnull().sum().sort_values(ascending=False)\ntotal_missing_values_X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing values in our independent and dependent variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking independent variable out of X and assigning to y\ny=X[['Cover_Type']]\nX=X.drop(columns=['Cover_Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Different Classifier Models and Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#**Commenting data scaling as scores are improved without data scaling ***\n# Feature Scaling training set for better predictions \n#from sklearn.preprocessing import StandardScaler\n#sc = StandardScaler()\n#X = sc.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression(random_state=0)\nclassifier_lr.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Train set results\ny_pred_lr=classifier_lr.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_lr=confusion_matrix(y,y_pred_lr)\ncm_lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As confusion matrix is not giving as clear picture , let's try with scatter plot to compare between 'y' and 'y_pred_lr'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting y from series to array , to generate a graph for comparision with y_pred_\ny=y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional y and y_pred array into single dimension \ny=y.ravel()\ny_pred_lr=y_pred_lr.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for y and y_pred_ to create line plot\ndf_lr=pd.DataFrame({\"y\":y,\"y_pred_lr\":y_pred_lr})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating scatter plot for both values to see comparision between y and y_pred\nplt.figure(figsize=(25,10))\nax=sns.scatterplot(x=range(1,15121),y=df_lr['y'],color='red')\nax=sns.scatterplot(x=range(1,15121),y=df_lr['y_pred_lr'],color='blue')\nax.set_xscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****How to read above graph : ** Red points represents actual values i.e 'y' and blue dots represents predicted one i.e. 'y_pred_lr'. We are seeing only few red dots because most of the points are overlapped which means correct predictions(y=y_pred_lr) but the one which are not are not overlapped are seeing seprately means incorrect predictions . In other words **All visible red dots are incorrect predictions**.\n\nIf we zoom this graph *(which we can do by increasing values in **plt.figure(figsize=(25,10)**)* we can see more red dots\n\n**Note:** Since 'x' axis range is big as compare to 'y' axis we have converted it into log values to see maximum values in small scale\n\nSo this model is giving us few incorrect predictions .Let's try other model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting KNN classifier to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier_knn=KNeighborsClassifier(n_neighbors=5, metric = 'minkowski', p = 2)\nclassifier_knn.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Train set results\ny_pred_knn=classifier_lr.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional  y_pred array into single dimension \ny_pred_knn=y_pred_knn.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for y and y_pred_ to create line plot\ndf_knn=pd.DataFrame({\"y\":y,\"y_pred_knn\":y_pred_knn})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating scatter plot for both values to see comparision between y and y_pred\nplt.figure(figsize=(25,10))\nax=sns.scatterplot(x=range(1,15121),y=df_knn['y'],color='red')\nax=sns.scatterplot(x=range(1,15121),y=df_knn['y_pred_knn'],color='blue')\nax.set_xscale('log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_rf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Train set results\ny_pred_rf=classifier_rf.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional  y_pred array into single dimension \ny_pred_rf=y_pred_rf.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for y and y_pred_ to create line plot\ndf_rf=pd.DataFrame({\"y\":y,\"y_pred_rf\":y_pred_rf})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating scatter plot for both values to see comparision between y and y_pred\nplt.figure(figsize=(25,10))\nax=sns.scatterplot(x=range(1,15121),y=df_rf['y'],color='red')\nax=sns.scatterplot(x=range(1,15121),y=df_rf['y_pred_rf'],color='blue')\nax.set_xscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that above graph is with very less number of red dots , as compare to previous graphs this is very accurate predictions , so let's focus on this algorithm i.e. on ** Random Forest Classifier **.\n\nNow , let's evaluate the accuracies of this model with the help of K-Fold Corss Validation"},{"metadata":{},"cell_type":"markdown","source":"# 6. K-Fold Cross Validation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies_rf = cross_val_score(estimator = classifier_rf, X = X, y = y, cv = 10)\naccuracies_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating mean and standard deviation for random forest model\naccuracies_rf.mean()\naccuracies_rf.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean Accuracy is coming close to 75% and standard Devaition is also not that much (~5%) , still we can improve this model.\n\nLet's try grid search to hypertune the parameters"},{"metadata":{},"cell_type":"markdown","source":"# 7. Grid Search"},{"metadata":{},"cell_type":"markdown","source":"> *Changing below two cells to markdown to prevent execution at the time of commit because this particular code is taking long time to execute . So I have executed this seprately to get best parameters and commenting this part to avoid long execution time during commit and generating output.*"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Applying Grid Search to find the best model and the best parameters\nfrom sklearn.model_selection import GridSearchCV\n#Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 300, 500, 1000]\n}\ngrid_search = GridSearchCV(estimator = classifier_rf, param_grid = param_grid,cv = 3, n_jobs = -1)\ngrid_search = grid_search.fit(X, y)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Getting the best params\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nbest_accuracy\nbest_parameters"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Getting the best params\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nbest_accuracy\nbest_parameters"},{"metadata":{},"cell_type":"markdown","source":"Let's create a new classifier model with above parameters . In addition to above grid search results I have refererd following kernels for fine tunning \n*  https://www.kaggle.com/arateris/2-layer-k-fold-learning-forest-cover#Feature-removal\n*  https://www.kaggle.com/joshofg/pure-random-forest-hyperparameter-tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rf_new = RandomForestClassifier(n_estimators = 719,\n                                           bootstrap=False,\n                                           max_depth=464,\n                                           max_features=0.3,\n                                           min_samples_leaf=1,\n                                           min_samples_split=2,\n                                           random_state=42)\nclassifier_rf_new.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Train set results\ny_pred_rf_new=classifier_rf_new.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional  y_pred array into single dimension \ny_pred_rf_new=y_pred_rf_new.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for y and y_pred_ to create line plot\ndf_rf_new=pd.DataFrame({\"y\":y,\"y_pred_rf_new\":y_pred_rf_new})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating scatter plot for both values to see comparision between y and y_pred\nplt.figure(figsize=(25,10))\nax=sns.scatterplot(x=range(1,15121),y=df_rf_new['y'],color='red')\nax=sns.scatterplot(x=range(1,15121),y=df_rf_new['y_pred_rf_new'],color='blue')\nax.set_xscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With latest fine tuning of random forest classifier model red dots are negligible , let's check K-Fold Cross Validation for this model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying k-Fold Cross Validation\naccuracies_rf_new = cross_val_score(estimator = classifier_rf_new, X = X, y = y, cv = 10)\naccuracies_rf_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating mean and standard deviation for random forest model\naccuracies_rf_new.mean()\naccuracies_rf_new.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean Accuracy is coming close to 80% and standard Devaition is also not that much (~4%) , it's improved a lot after finetuning .\n\n> **Note**: As if now best score i.e. '0.77' is coming with this model only and with these parameters\n\n Let's try another algorithm called **XG-boost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing required library and creating XGboost classifier model\n#Refered above mentioned kernels for fine tuning XGB classifier model\nfrom xgboost import XGBClassifier\nclassifier_xgb=XGBClassifier(n_estimators = 719,\n                             max_depth = 10)\nclassifier_xgb.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Train set results\ny_pred_xgb=classifier_xgb.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional  y_pred array into single dimension \ny_pred_xgb=y_pred_xgb.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating data frame for y and y_pred_ to create line plot\ndf_xgb=pd.DataFrame({\"y\":y,\"y_pred_xgb\":y_pred_xgb})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating scatter plot for both values to see comparision between y and y_pred\nplt.figure(figsize=(25,10))\nax=sns.scatterplot(x=range(1,15121),y=df_xgb['y'],color='red')\nax=sns.scatterplot(x=range(1,15121),y=df_xgb['y_pred_xgb'],color='blue')\nax.set_xscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here also red dots are becoming negligible , let's evaluate the model with K-Cross fold validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies_xgb = cross_val_score(estimator = classifier_xgb, X = X, y = y, cv = 10)\naccuracies_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating mean and standard deviation for random forest model\naccuracies_xgb.std()\naccuracies_xgb.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean Accuracy is coming close to 79% and standard Devaition is also not that much (~4%).\n\nFor predictions from Test set let's take last three models which are random forest without tuned parameters, random forest with tuned parameters and XGboost"},{"metadata":{},"cell_type":"markdown","source":"# 8. Cover Type Prediction from Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get test data \ndataset_test = pd.read_csv(\"../input/learn-together/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create X_test and fetching id in different frame\nX_test=dataset_test.drop(columns=['Id'])\ny_test_id=dataset_test[['Id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting Id into array\ny_test_id=y_test_id.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional y_test_id into single dimension \ny_test_id=y_test_id.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking missing value in test data set\ntotal_missing_values_X_test=X_test.isnull().sum().sort_values(ascending=False)\ntotal_missing_values_X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing value in test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#**Commenting data scaling as scores are improved without data scaling ***\n#Scaling and Transforming test set also as train set is already scaled and transformed\n#X_test = sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating predictions from random forest model without fine tuned parameters\ny_test_pred_rf=classifier_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional y_test_pred into single dimension \ny_test_pred_rf=y_test_pred_rf.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Submission dataframe from id and predecited Sale price\nsubmission_df_rf=pd.DataFrame({\"Id\":y_test_id,\"Cover_Type\":y_test_pred_rf})\n#Setting index as Id Column\nsubmission_df_rf.set_index(\"Id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting into CSV file for submission\nsubmission_df_rf.to_csv(\"submission_rf.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating predictions from random forest model with fine tuned parameters\ny_test_pred_rf_new=classifier_rf_new.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional y_test_pred into single dimension \ny_test_pred_rf_new=y_test_pred_rf_new.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Submission dataframe from id and predecited Sale price\nsubmission_df_rf_new=pd.DataFrame({\"Id\":y_test_id,\"Cover_Type\":y_test_pred_rf_new})\n#Setting index as Id Column\nsubmission_df_rf_new.set_index(\"Id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting into CSV file for submission\nsubmission_df_rf_new.to_csv(\"submission_rf_new.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating predictions from XGB model\ny_test_pred_xgb=classifier_xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting 2 dimensional y_test_pred into single dimension \ny_test_pred_xgb=y_test_pred_xgb.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Submission dataframe from id and predecited Sale price\nsubmission_df_xgb=pd.DataFrame({\"Id\":y_test_id,\"Cover_Type\":y_test_pred_xgb})\n#Setting index as Id Column\nsubmission_df_xgb.set_index(\"Id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting into CSV file for submission\nsubmission_df_xgb.to_csv(\"submission_xgb.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}